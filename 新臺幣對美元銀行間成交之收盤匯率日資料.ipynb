{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from queue import Queue\n",
    "import re\n",
    "import threading\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl the page by giving url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlThread(threading.Thread):\n",
    "    def __init__(self, url, year, q, lock):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.url = url\n",
    "        self.year = year\n",
    "        self.q = q\n",
    "        self.lock = lock\n",
    "    \n",
    "    def run(self):\n",
    "        self.crawler_job(self.year, self.url, self.q, self.lock)\n",
    "        print(self.year, 'finish')\n",
    "    \n",
    "    def crawler_job(self, year, url, q, lock):\n",
    "        pattern = re.compile('(?P<year>\\d+)/(?P<month>\\d+)/(?P<day>\\d+)')\n",
    "        lock.acquire()\n",
    "        US_exchange_rate_data = OrderedDict()\n",
    "        try:\n",
    "            annual_page_content = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "            # Get the table of exchange rate\n",
    "            row_data = annual_page_content.find(class_='text12').find_all('tr')[1:]\n",
    "            for row in row_data:\n",
    "                date, NTD_USD = [r.text for r in row.find_all('td')]\n",
    "                date = re.sub('\\s|\\u3000', '', date)\n",
    "                if date == \"\":\n",
    "                    continue\n",
    "                if pattern.match(date) != None:\n",
    "                    month = '{:02d}'.format(int(pattern.match(date).group('month')))\n",
    "                    day = '{:02d}'.format(int(pattern.match(date).group('day')))\n",
    "                    year_and_month = '{}/{}'.format(year, month)\n",
    "                    # Replace the date format from '103/03/2' to '2014/03/2'\n",
    "                    date = '{}/{}'.format(year_and_month, day)\n",
    "                    US_exchange_rate_data[date] = NTD_USD\n",
    "                else:\n",
    "                    # Replace the date format from '2' to '2014/03/2'\n",
    "                    date = '{}/{:02d}'.format(year_and_month, int(date))\n",
    "                    US_exchange_rate_data[date] = NTD_USD\n",
    "        except AttributeError:\n",
    "            print(url)\n",
    "            print('Error')\n",
    "            \n",
    "        q.put(US_exchange_rate_data)\n",
    "        time.sleep(0.5)\n",
    "        lock.release()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    content = requests.get(url).text    \n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 finish\n",
      "2017 finish\n",
      "2016 finish\n",
      "2015 finish\n",
      "2014 finish\n",
      "2013 finish\n",
      "2012 finish\n",
      "2011 finish\n",
      "2010 finish\n",
      "2009 finish\n",
      "2008 finish\n",
      "https://www.cbc.gov.tw/ct.asp?xItem=2284&ctNode=382&mp=1\n",
      "Error\n",
      "2007 finish\n",
      "2006 finish\n",
      "2005 finish\n",
      "2004 finish\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = 'https://www.cbc.gov.tw/'\n",
    "US_exchange_rate_data = OrderedDict()\n",
    "\n",
    "url = 'https://www.cbc.gov.tw/lp.asp?ctNode=382&CtUnit=128&BaseDSD=7'\n",
    "page_content = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "# Get the page link of exchange rate of every year\n",
    "annual_page_link = page_content.find(class_='list').find_all('a')\n",
    "annual_data_url = ['{}{}'.format(BASE_URL, annual_link.get('href')) for annual_link in annual_page_link]\n",
    "annual_data_year = [int(annual_link.get('title')[:4]) for annual_link in annual_page_link]\n",
    "\n",
    "threads = []\n",
    "lock = threading.Lock()\n",
    "q = Queue()\n",
    "for year, url in zip(annual_data_year, annual_data_url):\n",
    "    t = CrawlThread(url, year, q, lock)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print('Done')\n",
    "for _ in range(len(annual_page_link)):\n",
    "    US_exchange_rate_data.update(q.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(US_exchange_rate_data, orient='index').to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
